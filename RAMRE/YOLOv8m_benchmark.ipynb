{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab970236-598c-407a-9601-8a4c9a8c504c",
   "metadata": {},
   "source": [
    "# FORSMITH Roof Image Classifier —  YOLOv8m\n",
    "\n",
    "**Objective:** Train a model that takes a roof image and predicts the correct `observation_id` (class) from the `forsmith_roof_labels.json` taxonomy.  \n",
    "**Artifacts:** `model_best.pt`, `label_map.json`, `calibration.json`, `metrics.json`, ONNX export (optional), and an inference wrapper with optional **sheet-aware** masking.\n",
    "\n",
    "> Dataset: 1,616 images. CSV columns required: `image_file`, `label`, `observation_id`. The filename contains `report_id` as `<report_id>_pageXX_imgY.png`, enabling **GroupKFold** by report to avoid leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc9fff-ae4e-47d1-ab2c-b3e416ab1174",
   "metadata": {},
   "source": [
    "**Section 0 – Dependency Installs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8feb44-7e2c-404c-b946-3ce2ec81ace6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.1.2\n",
      "Uninstalling numpy-2.1.2:\n",
      "  Successfully uninstalled numpy-2.1.2\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch==2.5.1 in /opt/conda/lib/python3.10/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /opt/conda/lib/python3.10/site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio==2.5.1 in /opt/conda/lib/python3.10/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (4.15.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (1.13.1)\n",
      "Collecting numpy (from torchvision==0.20.1)\n",
      "  Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.20.1) (12.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.1) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.5.1) (3.0.3)\n",
      "Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ultralytics 8.2.103 requires numpy<2.0.0,>=1.23.0, but you have numpy 2.1.2 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.1.2\n",
      "/bin/bash: line 1: 2.2: No such file or directory\n",
      "torch: 2.5.1+cu121\n",
      "torchvision: 0.20.1+cu121\n",
      "torchaudio: 2.5.1+cu121\n",
      "numpy: 2.1.2\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Section 0 - Dependency Installs\n",
    "# Ensures all required Python packages are available inside the environment.\n",
    "# Torch 2.5.1 (CUDA 12.1) + Ultralytics YOLOv8 + Scikit-learn + Pandas + OpenCV.\n",
    "\n",
    "# %% [code]\n",
    "!pip uninstall -y numpy || true\n",
    "!pip install --index-url https://download.pytorch.org/whl/cu121 torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1\n",
    "!pip install -q numpy<2.2 ultralytics==8.2.103 scikit-learn opencv-python tqdm matplotlib pandas\n",
    "\n",
    "import torch, torchvision, torchaudio, numpy as np\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"torchaudio:\", torchaudio.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d421fe-9bfb-4887-9b16-3b4ef7865c38",
   "metadata": {},
   "source": [
    "**Section 1 – Dataset Load + Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8ac21a-f868-4ab2-b9db-1b3ed6b7f193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ready\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"/home/jupyter/forsmith_roof_data\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"DATA_ROOT\": str(DATA_ROOT),\n",
    "    \"IMAGES_DIR\": str(DATA_ROOT / \"images\"),\n",
    "    \"CSV_PATH\": str(DATA_ROOT / \"labels.csv\"),\n",
    "    \"LABELS_JSON\": str(DATA_ROOT / \"forsmith_roof_labels.json\"),\n",
    "\n",
    "    \"MODEL_NAME\": \"yolov8m-cls.pt\",\n",
    "    \"EPOCHS\": 60,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"IMAGE_SIZE\": 512,\n",
    "    \"LEARNING_RATE\": 5e-4,\n",
    "    \"PATIENCE\": 10,\n",
    "    \"OPTIMIZER\": \"Adam\",\n",
    "    \"LOSS_TYPE\": \"cross_entropy\",\n",
    "    \"DROPOUT\": 0.3,\n",
    "\n",
    "    \"N_SPLITS\": 5,\n",
    "    \"FOLD_INDEX\": 0,\n",
    "    \"SEED\": 1337,\n",
    "\n",
    "    \"OUT_DIR\": str(DATA_ROOT / \"outputs\" / \"yolov8m\"),\n",
    "    \"SAVE_ON_BEST\": \"val_acc\",\n",
    "}\n",
    "\n",
    "print(\"config ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8721b17-e584-4732-a5ee-caf0f8436edf",
   "metadata": {},
   "source": [
    "**Section 2 – Dataset Load + Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e263e8af-47d8-42e9-99fe-4e3a3759fc05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 label counts:\n",
      "observation_id\n",
      "1.01.05    126\n",
      "1.10.03     89\n",
      "1.06.04     78\n",
      "1.10.04     69\n",
      "1.07.01     66\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: 1292 | Val: 324 | Classes: 121\n",
      "\n",
      "Full dataframe sample:\n",
      "                  image_file                                   label  \\\n",
      "0  18-053-12_page12_img2.png                    Unprotected Openings   \n",
      "1  18-053-12_page17_img3.png             Redundant roof penetrations   \n",
      "2   23-023R1_page20_img2.png                     Subsurface Moisture   \n",
      "3    23-023R1_page5_img3.png  Conduit Penetration Through Mech. Unit   \n",
      "4     21-009_page18_img2.png  Conduit Penetration Through Mech. Unit   \n",
      "\n",
      "  observation_id  confidence  \\\n",
      "0        2.11.02       0.502   \n",
      "1        2.06.01       0.504   \n",
      "2        2.12.01       0.505   \n",
      "3        2.04.04       0.506   \n",
      "4        2.04.04       0.506   \n",
      "\n",
      "                                          image_path  obs_id  \n",
      "0  /home/jupyter/forsmith_roof_data/images/18-053...      70  \n",
      "1  /home/jupyter/forsmith_roof_data/images/18-053...      55  \n",
      "2  /home/jupyter/forsmith_roof_data/images/23-023...      71  \n",
      "3  /home/jupyter/forsmith_roof_data/images/23-023...      54  \n",
      "4  /home/jupyter/forsmith_roof_data/images/21-009...      54  \n",
      "\n",
      "Training set sample:\n",
      "                  image_file                                   label  \\\n",
      "0  18-053-12_page12_img2.png                    Unprotected Openings   \n",
      "1  18-053-12_page17_img3.png             Redundant roof penetrations   \n",
      "2   23-023R1_page20_img2.png                     Subsurface Moisture   \n",
      "3    23-023R1_page5_img3.png  Conduit Penetration Through Mech. Unit   \n",
      "4     21-009_page18_img2.png  Conduit Penetration Through Mech. Unit   \n",
      "\n",
      "  observation_id  confidence  \\\n",
      "0        2.11.02       0.502   \n",
      "1        2.06.01       0.504   \n",
      "2        2.12.01       0.505   \n",
      "3        2.04.04       0.506   \n",
      "4        2.04.04       0.506   \n",
      "\n",
      "                                          image_path  obs_id  \n",
      "0  /home/jupyter/forsmith_roof_data/images/18-053...      70  \n",
      "1  /home/jupyter/forsmith_roof_data/images/18-053...      55  \n",
      "2  /home/jupyter/forsmith_roof_data/images/23-023...      71  \n",
      "3  /home/jupyter/forsmith_roof_data/images/23-023...      54  \n",
      "4  /home/jupyter/forsmith_roof_data/images/21-009...      54  \n",
      "\n",
      "Validation set sample:\n",
      "                  image_file                         label observation_id  \\\n",
      "14    17-079_page23_img2.png           Subsurface Moisture        2.12.01   \n",
      "16  18-053-5_page13_img3.png   Redundant roof penetrations        2.06.01   \n",
      "35    18-068_page10_img1.png           Subsurface Moisture        2.12.01   \n",
      "36    18-068_page17_img2.png                  Pitch Pocket        2.06.03   \n",
      "40    18-068_page14_img2.png  Missing/Backed Out Fasteners        2.04.02   \n",
      "\n",
      "    confidence                                         image_path  obs_id  \n",
      "14       0.521  /home/jupyter/forsmith_roof_data/images/17-079...      71  \n",
      "16       0.522  /home/jupyter/forsmith_roof_data/images/18-053...      55  \n",
      "35       0.547  /home/jupyter/forsmith_roof_data/images/18-068...      71  \n",
      "36       0.548  /home/jupyter/forsmith_roof_data/images/18-068...      57  \n",
      "40       0.550  /home/jupyter/forsmith_roof_data/images/18-068...      53  \n",
      "\n",
      "Missing images: 0\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 2) DATASET LOAD + PREPROCESSING\n",
    "# ===============================\n",
    "import os, json, shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Load labels CSV\n",
    "df = pd.read_csv(CONFIG[\"CSV_PATH\"])\n",
    "df[\"image_path\"] = df[\"image_file\"].apply(\n",
    "    lambda x: os.path.join(CONFIG[\"IMAGES_DIR\"], x)\n",
    ")\n",
    "\n",
    "# Encode observation_id → numeric class\n",
    "obs_to_id = {obs: i for i, obs in enumerate(sorted(df[\"observation_id\"].unique()))}\n",
    "df[\"obs_id\"] = df[\"observation_id\"].map(obs_to_id)\n",
    "\n",
    "# Print summary\n",
    "print(\"Top 5 label counts:\")\n",
    "print(df[\"observation_id\"].value_counts().head())\n",
    "\n",
    "# GroupKFold by report prefix (e.g., 18-053)\n",
    "groups = df[\"image_file\"].str.split(\"_\").str[0]\n",
    "gkf = GroupKFold(n_splits=CONFIG[\"N_SPLITS\"])\n",
    "train_idx, val_idx = next(gkf.split(df, groups=groups))\n",
    "train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df)} | Val: {len(val_df)} | Classes: {len(obs_to_id)}\")\n",
    "\n",
    "# ===== PREVIEW DATASETS =====\n",
    "print(\"\\nFull dataframe sample:\")\n",
    "print(df.head(5))\n",
    "\n",
    "print(\"\\nTraining set sample:\")\n",
    "print(train_df.head(5))\n",
    "\n",
    "print(\"\\nValidation set sample:\")\n",
    "print(val_df.head(5))\n",
    "\n",
    "# Optional sanity check — ensure image paths exist\n",
    "missing_imgs = df[~df[\"image_path\"].apply(os.path.exists)]\n",
    "print(f\"\\nMissing images: {len(missing_imgs)}\")\n",
    "if len(missing_imgs) > 0:\n",
    "    print(missing_imgs.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead134a-3cde-4bd9-abd6-1a61510d10b3",
   "metadata": {},
   "source": [
    "**Section 3 – YOLO Folder Structure & YAML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4b345c-6b25-4e29-9197-f2c6f3f6a281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ YOLO directory structure created!\n",
      "Train samples: 1292\n",
      "Val samples: 324\n",
      "\n",
      "✅ YAML file written at: /home/jupyter/forsmith_roof_data/roof_yolov8.yaml\n",
      "\n",
      "--- YAML CONTENTS ---\n",
      "train: /home/jupyter/forsmith_roof_data/yolo_split/train\n",
      "val: /home/jupyter/forsmith_roof_data/yolo_split/val\n",
      "names:\n",
      "  0: '1.01.01'\n",
      "  1: '1.01.02'\n",
      "  2: '1.01.03'\n",
      "  3: '1.01.05'\n",
      "  4: '1.02.01'\n",
      "  5: '1.02.03'\n",
      "  6: '1.02.04'\n",
      "  7: '1.02.05'\n",
      "  8: '1.02.06'\n",
      "  9: '1.02.07'\n",
      "  10: '1.03.01'\n",
      "  11: '1.03.02'\n",
      "  12: '1.03.03'\n",
      "  13: '1.03.04'\n",
      "  14: '1.04.02'\n",
      "  15: '1.04.03'\n",
      "  16: '1.04.04'\n",
      "  17: '1.04.05'\n",
      "  18: '1.04.06'\n",
      "  19: '1.05.01'\n",
      "  20: '1.05.02'\n",
      "  21: '1.05.03'\n",
      "  22: '1.06.01'\n",
      "  23: '1.06.02'\n",
      "  24: '1.06.03'\n",
      "  25: '1.06.04'\n",
      "  26: '1.06.06'\n",
      "  27: '1.07.01'\n",
      "  28: '1.07.02'\n",
      "  29: '1.07.03'\n",
      "  30: '1.07.04'\n",
      "  31: '1.07.05'\n",
      "  32: '1.07.06'\n",
      "  33: '1.07.07'\n",
      "  34: '1.08.01'\n",
      "  35: '1.08.02'\n",
      "  36: '1.08.03'\n",
      "  37: '1.08.04'\n",
      "  38: '1.09.01'\n",
      "  39: '1.09.02'\n",
      "  40: '1.09.03'\n",
      "  41: '1.09.04'\n",
      "  42: '1.09.05'\n",
      "  43: '1.10.01'\n",
      "  44: '1.10.02'\n",
      "  45: '1.10.03'\n",
      "  46: '1.10.04'\n",
      "  47: '1.10.05'\n",
      "  48: '1.10.06'\n",
      "  49: '1.11.02'\n",
      "  50: '1.12.01'\n",
      "  51: '1.12.02'\n",
      "  52: '2.03.02'\n",
      "  53: '2.04.02'\n",
      "  54: '2.04.04'\n",
      "  55: '2.06.01'\n",
      "  56: '2.06.02'\n",
      "  57: '2.06.03'\n",
      "  58: '2.07.01'\n",
      "  59: '2.07.02'\n",
      "  60: '2.07.05'\n",
      "  61: '2.07.06'\n",
      "  62: '2.07.07'\n",
      "  63: '2.08.02'\n",
      "  64: '2.08.03'\n",
      "  65: '2.08.04'\n",
      "  66: '2.09.01'\n",
      "  67: '2.10.02'\n",
      "  68: '2.10.05'\n",
      "  69: '2.10.06'\n",
      "  70: '2.11.02'\n",
      "  71: '2.12.01'\n",
      "  72: '2.12.02'\n",
      "  73: '3.02.01'\n",
      "  74: '3.04.06'\n",
      "  75: '3.06.01'\n",
      "  76: '3.06.02'\n",
      "  77: '3.06.03'\n",
      "  78: '3.07.05'\n",
      "  79: '3.08.02'\n",
      "  80: '3.08.03'\n",
      "  81: '3.08.05'\n",
      "  82: '3.09.03'\n",
      "  83: '3.10.02'\n",
      "  84: '3.12.01'\n",
      "  85: '3.12.02'\n",
      "  86: '4.08.01'\n",
      "  87: '4.08.02'\n",
      "  88: '4.10.06'\n",
      "  89: '4.10.07'\n",
      "  90: '4.12.02'\n",
      "  91: '5.07.05'\n",
      "  92: '5.07.07'\n",
      "  93: '5.12.02'\n",
      "  94: '6.01.01'\n",
      "  95: '6.01.02'\n",
      "  96: '6.01.03'\n",
      "  97: '6.01.04'\n",
      "  98: '6.01.05'\n",
      "  99: '6.03.02'\n",
      "  100: '6.03.03'\n",
      "  101: '6.03.04'\n",
      "  102: '6.04.02'\n",
      "  103: '6.04.03'\n",
      "  104: '6.04.04'\n",
      "  105: '6.04.05'\n",
      "  106: '6.05.01'\n",
      "  107: '6.05.03'\n",
      "  108: '6.05.06'\n",
      "  109: '6.06.01'\n",
      "  110: '6.06.02'\n",
      "  111: '6.06.03'\n",
      "  112: '6.06.04'\n",
      "  113: '6.09.01'\n",
      "  114: '6.10.01'\n",
      "  115: '6.10.02'\n",
      "  116: '7.04.01'\n",
      "  117: '7.06.02'\n",
      "  118: '7.06.03'\n",
      "  119: '7.09.01'\n",
      "  120: '7.10.02'\n",
      "\n",
      "\n",
      "Train folder exists: True\n",
      "Val folder exists: True\n",
      "Example classes: ['63', '67', '93', '112', '49']\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 3) YOLO DATASET STRUCTURE (FIXED)\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import shutil, os\n",
    "\n",
    "# Root paths\n",
    "root = Path(CONFIG[\"DATA_ROOT\"])\n",
    "split_root = root / \"yolo_split\"\n",
    "train_dir, val_dir = split_root / \"train\", split_root / \"val\"\n",
    "\n",
    "# 🧹 Clean up old folders (if they exist)\n",
    "if split_root.exists():\n",
    "    shutil.rmtree(split_root)\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 🏗️ Create class folders and copy images\n",
    "for split_name, split_df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "    split_path = train_dir if split_name == \"train\" else val_dir\n",
    "    for _, row in split_df.iterrows():\n",
    "        cls_dir = split_path / str(row[\"obs_id\"])\n",
    "        cls_dir.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(row[\"image_path\"], cls_dir)\n",
    "\n",
    "print(\"✅ YOLO directory structure created!\")\n",
    "print(\"Train samples:\", sum(len(files) for _, _, files in os.walk(train_dir)))\n",
    "print(\"Val samples:\", sum(len(files) for _, _, files in os.walk(val_dir)))\n",
    "\n",
    "# 🧾 Create YAML file OUTSIDE yolo_split folder\n",
    "yaml_path = root / \"roof_yolov8.yaml\"\n",
    "\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    f.write(f\"train: {train_dir.resolve()}\\n\")\n",
    "    f.write(f\"val: {val_dir.resolve()}\\n\")\n",
    "    f.write(\"names:\\n\")\n",
    "    for i, obs in enumerate(sorted(obs_to_id.keys())):\n",
    "        f.write(f\"  {i}: '{obs}'\\n\")\n",
    "\n",
    "# ✅ Verify YAML and paths\n",
    "print(\"\\n✅ YAML file written at:\", yaml_path)\n",
    "print(\"\\n--- YAML CONTENTS ---\")\n",
    "print(open(yaml_path).read())\n",
    "\n",
    "print(\"\\nTrain folder exists:\", os.path.isdir(train_dir))\n",
    "print(\"Val folder exists:\", os.path.isdir(val_dir))\n",
    "print(\"Example classes:\", os.listdir(train_dir)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c28330-31fa-446e-9329-a5e7ffd6a680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders ready with strong augmentations and fixed class mapping.\n",
      "Train images: 1292 | Val images: 315\n",
      "Classes detected: 114\n",
      "Example classes: ['0', '1', '10', '100', '101']\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 3.5) PyTorch Dataset & Dataloaders (Fixed mapping + Strong Augmentation for Small Data)\n",
    "# ==============================\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets.folder import default_loader, IMG_EXTENSIONS\n",
    "from pathlib import Path\n",
    "\n",
    "train_root = \"/home/jupyter/forsmith_roof_data/yolo_split/train\"\n",
    "val_root   = \"/home/jupyter/forsmith_roof_data/yolo_split/val\"\n",
    "\n",
    "# 1️⃣ Build a fixed, global class mapping from the training set\n",
    "all_classes = sorted([d.name for d in Path(train_root).iterdir() if d.is_dir()])\n",
    "class_to_idx = {c: i for i, c in enumerate(all_classes)}\n",
    "\n",
    "def make_fixed_samples(root, class_to_idx):\n",
    "    samples = []\n",
    "    for cls in all_classes:  # enforce fixed order\n",
    "        d = Path(root) / cls\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        for p in d.rglob(\"*\"):\n",
    "            if p.suffix.lower() in IMG_EXTENSIONS:\n",
    "                samples.append((str(p), class_to_idx[cls]))\n",
    "    return samples\n",
    "\n",
    "# 2️⃣ Dataset subclass that reuses the fixed mapping\n",
    "class FixedImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform, samples, class_to_idx, classes):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.classes = classes\n",
    "        self.samples = samples\n",
    "        self.targets = [s[1] for s in samples]\n",
    "\n",
    "# 3️⃣ Strong augmentations (tuned for small roof datasets)\n",
    "# Includes geometric + color + erasing + perspective\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "    ], p=0.9),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.RandomResizedCrop(512, scale=(0.6, 1.0), ratio=(0.8, 1.25)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))], p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.15))\n",
    "])\n",
    "\n",
    "# Validation transform (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 4️⃣ Build datasets\n",
    "train_samples = make_fixed_samples(train_root, class_to_idx)\n",
    "val_samples   = make_fixed_samples(val_root, class_to_idx)\n",
    "\n",
    "train_data = FixedImageFolder(train_root, train_transform, train_samples, class_to_idx, all_classes)\n",
    "val_data   = FixedImageFolder(val_root, val_transform, val_samples, class_to_idx, all_classes)\n",
    "\n",
    "# 5️⃣ DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_data,   batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 6️⃣ Verify setup\n",
    "print(\"✅ DataLoaders ready with strong augmentations and fixed class mapping.\")\n",
    "print(f\"Train images: {len(train_data)} | Val images: {len(val_data)}\")\n",
    "print(f\"Classes detected: {len(train_data.classes)}\")\n",
    "print(\"Example classes:\", train_data.classes[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2d9f02-ee25-461e-bc7d-8789e83682cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_data.classes == val_data.classes)\n",
    "\n",
    "print(train_data.class_to_idx == val_data.class_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf60d6b-a84e-41bf-b093-62053b42bd6a",
   "metadata": {},
   "source": [
    "**Section 4 – Train YOLOv8m**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e01de-dc84-401d-9085-c7ee084b6eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 121 classes from YAML.\n",
      "Trainable params: 93,049 | Frozen params: 14,786,736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Epoch Progress Log\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60:  37%|███▋      | 15/41 [03:24<05:28, 12.63s/it]"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 4) YOLOv8 BACKBONE + TRAIN LOOP\n",
    "#    (Dual-Axis Graph + Full Epoch Logging, EMA, Cosine LR)\n",
    "# ================================\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import yaml, os, copy\n",
    "from math import cos, pi\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Load pretrained YOLOv8 backbone\n",
    "yolo_model = YOLO(CONFIG[\"MODEL_NAME\"])\n",
    "backbone = yolo_model.model.model[:-1]  # strip YOLO detection head\n",
    "\n",
    "# 2) Freeze backbone first\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 2.5) Partially unfreeze top layers (tune as needed)\n",
    "for name, p in backbone.named_parameters():\n",
    "    if any(k in name.lower() for k in [\"stage4\", \"stage5\", \"sppf\", \"head\", \"neck\", \"c5\", \"p4\", \"p5\"]):\n",
    "        p.requires_grad = True\n",
    "\n",
    "# Reactivate BatchNorm stats updating\n",
    "for m in backbone.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.train()\n",
    "\n",
    "# 3) Classification head\n",
    "yaml_path = \"/home/jupyter/forsmith_roof_data/roof_yolov8.yaml\"\n",
    "with open(yaml_path, \"r\") as f:\n",
    "    names_yaml = yaml.safe_load(f)\n",
    "num_classes = len(names_yaml[\"names\"])\n",
    "print(f\"Detected {num_classes} classes from YAML.\")\n",
    "\n",
    "classifier_head = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(768, num_classes)  # 768 works for yolov8m-cls backbone output\n",
    ")\n",
    "\n",
    "model = nn.Sequential(backbone, classifier_head).to(device)\n",
    "\n",
    "# 4) Optimizer with two parameter groups\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": [p for p in backbone.parameters() if p.requires_grad], \"lr\": CONFIG[\"LEARNING_RATE\"] * 0.2},\n",
    "    {\"params\": classifier_head.parameters(), \"lr\": CONFIG[\"LEARNING_RATE\"]}\n",
    "])\n",
    "epochs = CONFIG[\"EPOCHS\"]\n",
    "\n",
    "# 5) Cosine warmup scheduler (step per batch)\n",
    "def cosine_warmup_scheduler(optimizer, total_steps, warmup_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return 0.5 * (1.0 + cos(pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "warmup_steps = int(0.03 * total_steps)\n",
    "scheduler = cosine_warmup_scheduler(optimizer, total_steps, warmup_steps)\n",
    "\n",
    "# 6) EMA\n",
    "ema_decay = 0.999\n",
    "ema_model = copy.deepcopy(model).to(device)\n",
    "for p in ema_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def update_ema(ema_m, m, decay):\n",
    "    with torch.no_grad():\n",
    "        for (k, v_ema) in ema_m.state_dict().items():\n",
    "            v = m.state_dict()[k]\n",
    "            v_ema.copy_(decay * v_ema + (1.0 - decay) * v)\n",
    "\n",
    "# Early stopping & checkpoints\n",
    "patience = CONFIG.get(\"PATIENCE\", 10)\n",
    "best_acc, best_epoch = 0.0, 0\n",
    "save_dir = CONFIG[\"OUT_DIR\"]\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_path = os.path.join(save_dir, \"best_yolo_csv.pt\")\n",
    "ckpt_path = os.path.join(save_dir, \"checkpoint_latest.pt\")\n",
    "\n",
    "# Sanity check: trainable vs frozen\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(f\"Trainable params: {trainable:,} | Frozen params: {frozen:,}\")\n",
    "\n",
    "# Metric history\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "# Prepare a persistent display handle for the live chart (won't clear text logs)\n",
    "plot_handle = display(None, display_id=True)\n",
    "print(\"\\n📊 Epoch Progress Log\\n\" + \"-\" * 72)\n",
    "\n",
    "# ======================\n",
    "# TRAINING LOOP\n",
    "# ======================\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        # optional: label smoothing to reduce overconfidence on noisy labels\n",
    "        loss = F.cross_entropy(outputs, labels, label_smoothing=0.05)\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        update_ema(ema_model, model, ema_decay)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / max(1, total)\n",
    "    avg_loss = total_loss / max(1, len(train_loader))\n",
    "\n",
    "    # Validation (use EMA weights)\n",
    "    ema_model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = ema_model(imgs)\n",
    "            val_correct += (preds.argmax(1) == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    val_acc = val_correct / max(1, val_total)\n",
    "\n",
    "    # Checkpoints / early stopping\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(ema_model.state_dict(), best_path)\n",
    "    elif epoch - best_epoch >= patience:\n",
    "        print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model\": ema_model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"best_acc\": best_acc\n",
    "    }, ckpt_path)\n",
    "\n",
    "    # Log history\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    # ---- Print line for this epoch (kept; not cleared) ----\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "          f\"LR: {current_lr:.2e} | \"\n",
    "          f\"Train Loss: {avg_loss:.4f} | \"\n",
    "          f\"Train Acc: {train_acc*100:.2f}% | \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}% | \"\n",
    "          f\"Best: {best_acc*100:.2f}%\")\n",
    "\n",
    "    # ---- Update live dual-axis plot without clearing logs ----\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\", color=\"tab:red\")\n",
    "    ax1.plot(history[\"epoch\"], history[\"train_loss\"], color=\"tab:red\", linewidth=2, label=\"Train Loss\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"Accuracy (%)\", color=\"tab:blue\")\n",
    "    ax2.plot(history[\"epoch\"], [a * 100 for a in history[\"train_acc\"]],\n",
    "             color=\"tab:blue\", linewidth=2, label=\"Train Acc\")\n",
    "    ax2.plot(history[\"epoch\"], [a * 100 for a in history[\"val_acc\"]],\n",
    "             color=\"tab:green\", linewidth=2, label=\"Val Acc\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    plt.title(f\"Training Progress (Best Val Acc: {best_acc*100:.2f}%)\")\n",
    "    # combine legends from both axes\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    fig.legend(lines1 + lines2, labels1 + labels2,\n",
    "               loc=\"upper center\", bbox_to_anchor=(0.5, -0.10), ncol=3)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # show/update plot in place, but keep text output intact\n",
    "    plot_handle.update(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ======================\n",
    "# FINAL RESULTS\n",
    "# ======================\n",
    "plot_path = os.path.join(save_dir, \"training_curve.png\")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\", color=\"tab:red\")\n",
    "ax1.plot(history[\"epoch\"], history[\"train_loss\"], color=\"tab:red\", linewidth=2, label=\"Train Loss\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Accuracy (%)\", color=\"tab:blue\")\n",
    "ax2.plot(history[\"epoch\"], [a * 100 for a in history[\"train_acc\"]],\n",
    "         color=\"tab:blue\", linewidth=2, label=\"Train Acc\")\n",
    "ax2.plot(history[\"epoch\"], [a * 100 for a in history[\"val_acc\"]],\n",
    "         color=\"tab:green\", linewidth=2, label=\"Val Acc\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "plt.title(f\"Final Training Curve (Best Val Acc: {best_acc*100:.2f}%)\")\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "fig.legend(lines1 + lines2, labels1 + labels2,\n",
    "           loc=\"upper center\", bbox_to_anchor=(0.5, -0.10), ncol=3)\n",
    "fig.tight_layout()\n",
    "plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📈 Saved final training curve to: {plot_path}\")\n",
    "print(f\"💾 Latest checkpoint: {ckpt_path}\")\n",
    "print(f\"🎯 Training complete! Best Validation Accuracy: {best_acc:.4f}\")\n",
    "print(\"✅ Best model saved to:\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f6131-69ab-43a4-8f42-8cfc3a384412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
